Generally in the GeoSpatial world products produced sistematically, planned or discovered via the catalogue client are ordered and when ready have to be downloaded, pre-processed, stored on the reference server and archived for later discovery and access. This is one of the main functionalities of the Archiving and Resource Management Service (ARMS) tool component that address both the short-term and long-term archiving of the input data. The ARMS also provide the functionalities to retrieve previously stored datasets and to delete them. In addition the ARMS is also in charge of managing the metadata associated to the archived data. 

It can be seen as middleware software that encapsulates the publishing of data and metadata to a persistent Web accessible store (HTTP, FTP, WCS, WMS, WFS or SOS) and allows managing the stored resources. The ARMS can retrieve the data either by watching a local file system as well as by fetching the data from a remote location. In the former case is the data provider that has to push the data on the local file system via FTP. In the latter case the link to the data has to be provided via REST (typically via a web client client) or WPS (typically by a Workflow) interface. We can envisage two scenarios:

1.	The data is stored on the Sevice Provider side. In this scenario the link to the data to be downloaded is passed to the ARMS by a web  client via REST interface if the ingestion is triggered by the end user  or by the RCOMS server via WPS SOAP interface in case the download can be started automatically without the operator intervention.
2.	The data is pushed by the Service Provider to a specific FTP server account. In this case the ARMS watches for the data in a local folder linked with the FTP account and stores all files uploaded automatically.

In both cases the ARMS can be configured in order to preprocess the data before ingesting it and storing (Loading) it in the reference server (or FTP server depending on the data type). ARMS allows configuring the list of pre-processing available. Then it will be possible to define a processing chain for each watched directory or for each new type of data to be managed.

Data extracted and/or pre-processed can then be stored in one of the server connected with the ARMS (FTP, HTTP, WCS, WMS, WFS or SOS).

The GeoETL can be connected to a catalogue to allow the automatic publication of the metadata that can be provided as input (via WPS or REST interface) or extracted automatically during the extraction process. The harvesting is performed via standard CSW interface.

The data loaded in the system can then be managed via a graphical user interface that allows retrieving the instances associated to the data ingestion. From the GUI is then possible to see the ingestion logs and eventually delete the instances and undeploy the data. Undeplying the data means remove the data from all the servers and remove the entry from the catalogue, thus maintaining the system up to date and avoid broken links in the catalogue itself. Data deletion can be also performed at a pre-configured time. This allows implementing a rolling archiving.